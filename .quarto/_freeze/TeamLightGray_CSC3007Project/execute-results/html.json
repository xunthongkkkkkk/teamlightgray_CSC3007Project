{
  "hash": "b19a4ebcedd6d1ef10a91b39f4a2c0b1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Visualising Top 10 Sectors affected by Data Breaches (2010--2024)\nauthor: \"Team Lightgray\"\nformat: html\n---\n\n\n\n# Introduction\n\nThis document outlines the data engineering required to reconstruct and improve the plot by David McCandless, Tom Evans, and Paul Barton, published in [information_is_beautiful](https://informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks//), shown in @fig-databreach, depicting the world's biggest data breaches between 2004 and 2024. \n\nThe code below requires the following packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(readr)\n```\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Visualization of World's Biggest Data Breaches & Hacks from 2004 to 2024 by David McCandless, Tom Evans, and Paul Barton.](images/previous_visualization.png){#fig-databreach width=824}\n:::\n:::\n\n\n\n# Data Cleaning\n\nThe dataset includes records of data breaches with the following columns:\n\n*   `organization`: Name of the organization affected by the breach\n*   `records lost`: Number of records lost in the breach\n*   `year`: Year the breach occurred\n*   `sector`: Sector to which the organization belongs\n*   `method`: Method used in the breach\n*   `interesting story`: Additional information about the breach\n\nFirst, we read the data from the CSV file and clean it by removing any rows with missing values in critical columns such as records.lost, year, and sector.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- read_csv(\"IIB Data Breaches - LATEST - breaches.csv\")\n\n# Clean the data\ndata <- data %>%\n  mutate(\n    records.lost = as.numeric(gsub(\"[^0-9]\", \"\", `records lost`)),\n    year = as.numeric(year)\n  ) %>%\n  drop_na(records.lost, year, sector)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `year = as.numeric(year)`.\nCaused by warning:\n! NAs introduced by coercion\n```\n\n\n:::\n\n```{.r .cell-code}\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 460 × 17\n   organisation       `alternative name` `records lost`  year date  story sector\n   <chr>              <chr>              <chr>          <dbl> <chr> <chr> <chr> \n 1 AT&T               <NA>               73,000,000      2024 Apr … Sens… telec…\n 2 Irish towing comp… <NA>               512,000         2023 Oct … The … trans…\n 3 Maine Government   <NA>               1,300,000       2023 May … Russ… gover…\n 4 Welltok            <NA>               8,500,000       2023 Nov … Pati… health\n 5 Maximus            <NA>               10,000,000      2023 Jul … Expl… gover…\n 6 Okta               <NA>               134             2023 Nov … Name… tech  \n 7 Delta Dental       <NA>               7,000,000       2023 May … The … health\n 8 Xfinity            <NA>               36,000,000      2023 Oct … Hack… telec…\n 9 Atlassian          <NA>               13,200          2023 Feb … Sieg… tech  \n10 Reddit             <NA>               100,000         2023 Feb … A ph… web   \n# ℹ 450 more rows\n# ℹ 10 more variables: method <chr>, `interesting story` <chr>,\n#   `data sensitivity` <chr>, `displayed records` <chr>, ...12 <lgl>,\n#   `source name` <chr>, `1st source link` <chr>, `2nd source link` <chr>,\n#   ID <dbl>, records.lost <dbl>\n```\n\n\n:::\n:::\n\n\n## Grouping Sectors\n\nTo maintain relevance and reduce complexity, sectors with similar characteristics were merged into broader categories such as Web, Financial, Retail, and others. The emphasis was then refined to the top 10 sectors based on total records lost between 2010 and 2024.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Group sectors into broader categories\ndata <- data %>%\n  mutate(\n    sector_group = case_when(\n      grepl(\"web\", sector, ignore.case = TRUE) ~ \"Web\",\n      grepl(\"healthcare|health\", sector, ignore.case = TRUE) ~ \"Healthcare\",\n      grepl(\"app\", sector, ignore.case = TRUE) ~ \"App\",\n      grepl(\"retail\", sector, ignore.case = TRUE) ~ \"Retail\",\n      grepl(\"gaming\", sector, ignore.case = TRUE) ~ \"Gaming\",\n      grepl(\"transport\", sector, ignore.case = TRUE) ~ \"Transport\",\n      grepl(\"financial|finance\", sector, ignore.case = TRUE) ~ \"Financial\",\n      grepl(\"tech\", sector, ignore.case = TRUE) ~ \"Tech\",\n      grepl(\"government\", sector, ignore.case = TRUE) ~ \"Government\",\n      grepl(\"telecoms\", sector, ignore.case = TRUE) ~ \"Telecoms\",\n      grepl(\"legal\", sector, ignore.case = TRUE) ~ \"Legal\",\n      grepl(\"media\", sector, ignore.case = TRUE) ~ \"Media\",\n      grepl(\"academic\", sector, ignore.case = TRUE) ~ \"Academic\",\n      grepl(\"energy\", sector, ignore.case = TRUE) ~ \"Energy\",\n      grepl(\"military\", sector, ignore.case = TRUE) ~ \"Military\",\n      TRUE ~ \"Miscellaneous\"\n    )\n  ) %>%\n  drop_na(records.lost, year, sector_group)\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 460 × 18\n   organisation       `alternative name` `records lost`  year date  story sector\n   <chr>              <chr>              <chr>          <dbl> <chr> <chr> <chr> \n 1 AT&T               <NA>               73,000,000      2024 Apr … Sens… telec…\n 2 Irish towing comp… <NA>               512,000         2023 Oct … The … trans…\n 3 Maine Government   <NA>               1,300,000       2023 May … Russ… gover…\n 4 Welltok            <NA>               8,500,000       2023 Nov … Pati… health\n 5 Maximus            <NA>               10,000,000      2023 Jul … Expl… gover…\n 6 Okta               <NA>               134             2023 Nov … Name… tech  \n 7 Delta Dental       <NA>               7,000,000       2023 May … The … health\n 8 Xfinity            <NA>               36,000,000      2023 Oct … Hack… telec…\n 9 Atlassian          <NA>               13,200          2023 Feb … Sieg… tech  \n10 Reddit             <NA>               100,000         2023 Feb … A ph… web   \n# ℹ 450 more rows\n# ℹ 11 more variables: method <chr>, `interesting story` <chr>,\n#   `data sensitivity` <chr>, `displayed records` <chr>, ...12 <lgl>,\n#   `source name` <chr>, `1st source link` <chr>, `2nd source link` <chr>,\n#   ID <dbl>, records.lost <dbl>, sector_group <chr>\n```\n\n\n:::\n:::\n\n\n## Filtering Data\n\nWe filter the data to include only records from 2010 to 2024 and select the top 10 sectors based on the total records lost.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define the year range\nstart_year <- 2010\nend_year <- 2024\n\n# Filter the data based on the specified year range and minimum records lost\ndata <- data %>%\n  filter(year >= start_year & year <= end_year)\n\n# Calculate total records lost for each sector group and select top 10 sectors\ntop_sectors <- data %>%\n  group_by(sector_group) %>%\n  summarize(total_records_lost = sum(records.lost)) %>%\n  arrange(desc(total_records_lost)) %>%\n  top_n(10, wt = total_records_lost) %>%\n  pull(sector_group)\n\n# Filter the data to include only top 10 sectors\ndata <- data %>%\n  filter(sector_group %in% top_sectors)\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 393 × 18\n   organisation       `alternative name` `records lost`  year date  story sector\n   <chr>              <chr>              <chr>          <dbl> <chr> <chr> <chr> \n 1 AT&T               <NA>               73,000,000      2024 Apr … Sens… telec…\n 2 Irish towing comp… <NA>               512,000         2023 Oct … The … trans…\n 3 Maine Government   <NA>               1,300,000       2023 May … Russ… gover…\n 4 Welltok            <NA>               8,500,000       2023 Nov … Pati… health\n 5 Maximus            <NA>               10,000,000      2023 Jul … Expl… gover…\n 6 Okta               <NA>               134             2023 Nov … Name… tech  \n 7 Delta Dental       <NA>               7,000,000       2023 May … The … health\n 8 Xfinity            <NA>               36,000,000      2023 Oct … Hack… telec…\n 9 Atlassian          <NA>               13,200          2023 Feb … Sieg… tech  \n10 Reddit             <NA>               100,000         2023 Feb … A ph… web   \n# ℹ 383 more rows\n# ℹ 11 more variables: method <chr>, `interesting story` <chr>,\n#   `data sensitivity` <chr>, `displayed records` <chr>, ...12 <lgl>,\n#   `source name` <chr>, `1st source link` <chr>, `2nd source link` <chr>,\n#   ID <dbl>, records.lost <dbl>, sector_group <chr>\n```\n\n\n:::\n:::\n\n\n## Data Preparation for Visualization\n\nTo prepare the data for visualization, we convert the year column to a date format and add a new column to specify color based on sector importance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert the year to a date format for plotting\ndata <- data %>%\n  mutate(\n    StartDate = as.Date(paste(year, \"-01-01\", sep = \"\")),\n    EndDate = as.Date(paste(year, \"-12-31\", sep = \"\"))\n  )\n\n# Sort the sectors by total records lost\nsector_levels <- data %>%\n  group_by(sector_group) %>%\n  summarize(total_records_lost = sum(records.lost)) %>%\n  arrange(desc(total_records_lost)) %>%\n  pull(sector_group)\n\ndata <- data %>%\n  mutate(sector_group = factor(sector_group, levels = rev(sector_levels)))\n\n# Define important sectors to highlight\nhighlight_sectors <- c(\"Web\", \"Financial\", \"App\", \"Transport\")\n\n# Add a new column to specify color based on sector importance\ndata <- data %>%\n  mutate(\n    highlight = ifelse(as.character(sector_group) %in% highlight_sectors, as.character(sector_group), \"Other\")\n  )\n\n# Convert sector_group to a factor and then to numeric for plotting\ndata$sector_group_num <- as.numeric(factor(data$sector_group))\n\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 393 × 22\n   organisation       `alternative name` `records lost`  year date  story sector\n   <chr>              <chr>              <chr>          <dbl> <chr> <chr> <chr> \n 1 AT&T               <NA>               73,000,000      2024 Apr … Sens… telec…\n 2 Irish towing comp… <NA>               512,000         2023 Oct … The … trans…\n 3 Maine Government   <NA>               1,300,000       2023 May … Russ… gover…\n 4 Welltok            <NA>               8,500,000       2023 Nov … Pati… health\n 5 Maximus            <NA>               10,000,000      2023 Jul … Expl… gover…\n 6 Okta               <NA>               134             2023 Nov … Name… tech  \n 7 Delta Dental       <NA>               7,000,000       2023 May … The … health\n 8 Xfinity            <NA>               36,000,000      2023 Oct … Hack… telec…\n 9 Atlassian          <NA>               13,200          2023 Feb … Sieg… tech  \n10 Reddit             <NA>               100,000         2023 Feb … A ph… web   \n# ℹ 383 more rows\n# ℹ 15 more variables: method <chr>, `interesting story` <chr>,\n#   `data sensitivity` <chr>, `displayed records` <chr>, ...12 <lgl>,\n#   `source name` <chr>, `1st source link` <chr>, `2nd source link` <chr>,\n#   ID <dbl>, records.lost <dbl>, sector_group <fct>, StartDate <date>,\n#   EndDate <date>, highlight <chr>, sector_group_num <dbl>\n```\n\n\n:::\n:::\n\n\n# Conclusion\n\nThe data are now ready for visualization. The next step will be using the ggplot2 package to create a Gantt chart, that visualizes the timeline and impact of data breaches across the top 10 sectors from 2010 to 2024.\n\nBy grouping similar sectors, filtering for relevant data, and preparing the dataset for plotting, we can create a clear and insightful visualization that highlights the key trends and vulnerabilities in data breaches.\n\nIn the subsequent sections or documents, we can use this prepared data to generate the Gantt chart and further improve the visualization with additional insights and interactivity features.\n",
    "supporting": [
      "TeamLightGray_CSC3007Project_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}